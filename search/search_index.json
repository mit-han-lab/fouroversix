{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Four Over Six","text":"<p>See Quantization for details on quantizing tensors to FP4, and Matrix Multiplication for performing matrix multiplication with FP4 tensors.</p>"},{"location":"matmul/","title":"Matrix Multiplication","text":"<p>Perform a matrix multiplication (<code>a @ b.T</code>) with two FP4-quantized tensors provided in row-major layout.</p>"},{"location":"matmul/#fouroversix.fp4_matmul--sample-code","title":"Sample Code","text":"<p>Each tensor may be provided in either high or low precision. If provided in high precision, tensors will be quantized to FP4 prior to the matrix multiplication, and quantization may be configured with the <code>input_quantize_kwargs</code> and <code>other_quantize_kwargs</code> parameters. For example, the following two code samples are equivalent:</p>"},{"location":"matmul/#fouroversix.fp4_matmul--with-high-precision-inputs","title":"With High-Precision Inputs","text":"<pre><code>a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\nb = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\nout = fp4_matmul(a, b)\n</code></pre>"},{"location":"matmul/#fouroversix.fp4_matmul--with-low-precision-inputs","title":"With Low-Precision Inputs","text":"<pre><code>a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\nb = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n\na_quantized = quantize_to_fp4(a)\nb_quantized = quantize_to_fp4(b)\n\nout = fp4_matmul(a_quantized, b_quantized)\n</code></pre>"},{"location":"matmul/#fouroversix.fp4_matmul--backends","title":"Backends","text":"<p>We provide two different implementations of FP4 matrix multiplication:</p> <ul> <li>CUTLASS: Uses CUTLASS kernels to perform fast FP4 matrix multiplication.     Requires a Blackwell GPU.</li> <li>PyTorch: A slow implementation which dequantizes FP4 tensors, and then     performs a high-precision matrix multiplication.</li> </ul> <p>Note that our CUTLASS kernels accumulate in FP32, so it should be roughly equivalent to simulations done with the PyTorch backend.</p>"},{"location":"matmul/#fouroversix.fp4_matmul--parameters","title":"Parameters","text":"<p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor | FP4Tensor</code> <p>The first tensor to be multiplied.</p> required <code>other</code> <code>Tensor | FP4Tensor</code> <p>The second tensor to be multiplied.</p> required <code>backend</code> <code>MatmulBackend</code> <p>The backend to use for the matrix multiplication, either <code>MatmulBackend.cutlass</code> or <code>MatmulBackend.pytorch</code>. If no backend is provided, CUTLASS will be used if the machine has a Blackwell GPU, and PyTorch will be used otherwise.</p> <code>None</code> <code>input_quantize_kwargs</code> <code>dict</code> <p>If <code>a</code> is provided in high precision, these parameters will be passed to the <code>quantize_to_fp4</code> call done prior to the matrix multiplication.</p> <code>None</code> <code>other_quantize_kwargs</code> <code>dict</code> <p>If <code>other</code> is provided in high precision, these parameters will be passed to the <code>quantize_to_fp4</code> call done prior to the matrix multiplication.</p> <code>None</code> <code>out_dtype</code> <code>DataType</code> <p>The data type of the output tensor, either <code>DataType.bfloat16</code> or <code>DataType.float16</code>.</p> <code>bfloat16</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor.</p> Source code in <code>src/fouroversix/frontend.py</code> <pre><code>def fp4_matmul(\n    input: torch.Tensor | FP4Tensor,\n    other: torch.Tensor | FP4Tensor,\n    *,\n    backend: MatmulBackend | None = None,\n    input_quantize_kwargs: dict[str, Any] | None = None,\n    other_quantize_kwargs: dict[str, Any] | None = None,\n    out_dtype: torch.dtype = torch.bfloat16,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Perform a matrix multiplication (`a @ b.T`) with two FP4-quantized tensors provided\n    in row-major layout.\n\n    ## Sample Code\n\n    Each tensor may be provided in either high or low precision. If provided in high\n    precision, tensors will be quantized to FP4 prior to the matrix multiplication, and\n    quantization may be configured with the `input_quantize_kwargs` and\n    `other_quantize_kwargs` parameters. For example, the following two code samples are\n    equivalent:\n\n    ### With High-Precision Inputs\n\n    ```\n    a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n    b = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n    out = fp4_matmul(a, b)\n    ```\n\n    ### With Low-Precision Inputs\n\n    ```\n    a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n    b = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n\n    a_quantized = quantize_to_fp4(a)\n    b_quantized = quantize_to_fp4(b)\n\n    out = fp4_matmul(a_quantized, b_quantized)\n    ```\n\n    ## Backends\n\n    We provide two different implementations of FP4 matrix multiplication:\n\n    - **CUTLASS**: Uses CUTLASS kernels to perform fast FP4 matrix multiplication.\n        Requires a Blackwell GPU.\n    - **PyTorch**: A slow implementation which dequantizes FP4 tensors, and then\n        performs a high-precision matrix multiplication.\n\n    Note that our CUTLASS kernels accumulate in FP32, so it should be roughly\n    equivalent to simulations done with the PyTorch backend.\n\n    ## Parameters\n\n    Args:\n        input (torch.Tensor | FP4Tensor): The first tensor to be multiplied.\n        other (torch.Tensor | FP4Tensor): The second tensor to be multiplied.\n        backend (MatmulBackend): The backend to use for the matrix multiplication,\n            either `MatmulBackend.cutlass` or `MatmulBackend.pytorch`. If no backend is\n            provided, CUTLASS will be used if the machine has a Blackwell GPU, and\n            PyTorch will be used otherwise.\n        input_quantize_kwargs (dict): If `a` is provided in high precision, these\n            parameters will be passed to the `quantize_to_fp4` call done prior to the\n            matrix multiplication.\n        other_quantize_kwargs (dict): If `other` is provided in high precision, these\n            parameters will be passed to the `quantize_to_fp4` call done prior to the\n            matrix multiplication.\n        out_dtype (DataType): The data type of the output tensor, either\n            `DataType.bfloat16` or `DataType.float16`.\n\n    Returns:\n        The output tensor.\n\n    \"\"\"\n\n    if input_quantize_kwargs is None:\n        input_quantize_kwargs = {}\n\n    if other_quantize_kwargs is None:\n        other_quantize_kwargs = {}\n\n    if isinstance(input, torch.Tensor):\n        input = quantize_to_fp4(input, **(input_quantize_kwargs or {}))\n\n    if isinstance(other, torch.Tensor):\n        other = quantize_to_fp4(other, **(other_quantize_kwargs or {}))\n\n    if backend is None:\n        backend = MatmulBackend.auto_select()\n    elif not backend.is_available():\n        msg = f\"Backend {backend} is not available\"\n        raise ValueError(msg)\n\n    return backend.fp4_matmul(input, other, out_dtype=out_dtype)\n</code></pre>"},{"location":"quantization/","title":"Quantization","text":"<p>Quantize a tensor to FP4.</p>"},{"location":"quantization/#fouroversix.quantize_to_fp4--sample-code","title":"Sample Code","text":""},{"location":"quantization/#fouroversix.quantize_to_fp4--with-four-over-six","title":"With Four Over Six","text":"<pre><code>x = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\nx_quantized = quantize_to_fp4(x)\n</code></pre>"},{"location":"quantization/#fouroversix.quantize_to_fp4--without-four-over-six","title":"Without Four Over Six","text":"<pre><code>x = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\nx_quantized = quantize_to_fp4(x, scale_rule=AdaptiveBlockScalingRule.always_6)\n</code></pre>"},{"location":"quantization/#fouroversix.quantize_to_fp4--with-stochastic-rounding","title":"With Stochastic Rounding","text":"<pre><code>x = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\nx_quantized = quantize_to_fp4(x, round_style=RoundStyle.stochastic)\n</code></pre>"},{"location":"quantization/#fouroversix.quantize_to_fp4--with-the-random-hadamard-transform","title":"With the Random Hadamard Transform","text":"<pre><code>from fouroversix.quantize import get_rht_matrix\n\nx = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\nhad = get_rht_matrix()\nx_quantized = quantize_to_fp4(x, had=had)\n</code></pre>"},{"location":"quantization/#fouroversix.quantize_to_fp4--backends","title":"Backends","text":"<p>We provide three different implementations of FP4 quantization:</p> <ul> <li>CUDA: A fast implementation written in CUDA which currently does not support     the operations required for training (2D block scaling, stochastic rounding,     random Hadamard transform). Requires a Blackwell GPU.</li> <li>Triton: A slightly slower implementation written in Triton which supports all     operations needed for training. Requires a Blackwell GPU.</li> <li>PyTorch: A slow implementation written in PyTorch which supports all     operations and can be run on any GPU.</li> </ul> <p>If <code>quantize_to_fp4</code> is called with <code>backend=None</code>, a backend will be selected automatically based on the following rules:</p> <ul> <li>If there is no GPU available, or if the available GPU is not a Blackwell GPU,     select PyTorch.</li> <li>If any quantization options are set other than <code>scale_rule</code>, select Triton.<ul> <li>However, if the available GPU is SM120 (i.e. RTX 5090, RTX 6000) and     <code>round_style</code> is set to <code>RoundStyle.stochastic</code>, select PyTorch as     stochastic rounding does not have hardware support on SM120 GPUs.</li> </ul> </li> <li>Otherwise, select CUDA.</li> </ul>"},{"location":"quantization/#fouroversix.quantize_to_fp4--parameters","title":"Parameters","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to quantize.</p> required <code>backend</code> <code>QuantizeBackend</code> <p>The backend to use for quantization, either <code>QuantizeBackend.cuda</code>, <code>QuantizeBackend.triton</code>, or <code>QuantizeBackend.pytorch</code>. If no backend is provided, one will be selected automatically based on the available GPU and the options provided. See above for more details.</p> <code>None</code> <code>scale_rule</code> <code>AdaptiveBlockScalingRule</code> <p>The scaling rule to use during quantization. See (Adaptive Block Scaling)[/adaptive_block_scaling] for more details.</p> <code>mse</code> <code>block_scale_2d</code> <code>bool</code> <p>If True, scale factors will be computed across 16x16 chunks of the input rather than 1x16 chunks. This is useful to apply to the weight matrix during training, so that W and W.T will be equivalent after quantization.</p> <code>False</code> <code>had</code> <code>Tensor</code> <p>A high-precision Hadamard matrix to apply to the input prior to quantization.</p> <code>None</code> <code>fp4_format</code> <code>FP4Format</code> <p>The FP4 format to quantize to, either <code>FP4Format.mxfp4</code> or <code>FP4Format.nvfp4</code>.</p> <code>nvfp4</code> <code>round_style</code> <code>RoundStyle</code> <p>The rounding style to apply during quantization, either <code>RoundStyle.nearest</code> for round-to-nearest quantization, or <code>RoundStyle.stochastic</code> for stochastic rounding.</p> <code>nearest</code> <code>transpose</code> <code>bool</code> <p>If True, the output will be a quantized version of the transposed input. This may be helpful for certain operations during training as <code>fp4_matmul</code> requires that both tensors are provided in row-major format.</p> <code>False</code> <p>Returns:</p> Type Description <code>FP4Tensor</code> <p>A quantized FP4Tensor, which contains the packed E2M1 values, the FP8 scale</p> <code>FP4Tensor</code> <p>factors, and the tensor-wide FP32 scale factor.</p> Source code in <code>src/fouroversix/frontend.py</code> <pre><code>def quantize_to_fp4(\n    x: torch.Tensor,\n    *,\n    backend: QuantizeBackend | None = None,\n    scale_rule: AdaptiveBlockScalingRule = AdaptiveBlockScalingRule.mse,\n    block_scale_2d: bool = False,\n    had: torch.Tensor | None = None,\n    fp4_format: FP4Format = FP4Format.nvfp4,\n    round_style: RoundStyle = RoundStyle.nearest,\n    transpose: bool = False,\n) -&gt; FP4Tensor:\n    \"\"\"\n    Quantize a tensor to FP4.\n\n    ## Sample Code\n\n    ### With Four Over Six\n\n    ```\n    x = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n    x_quantized = quantize_to_fp4(x)\n    ```\n\n    ### Without Four Over Six\n\n    ```\n    x = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n    x_quantized = quantize_to_fp4(x, scale_rule=AdaptiveBlockScalingRule.always_6)\n    ```\n\n    ### With Stochastic Rounding\n\n    ```\n    x = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n    x_quantized = quantize_to_fp4(x, round_style=RoundStyle.stochastic)\n    ```\n\n    ### With the Random Hadamard Transform\n\n    ```\n    from fouroversix.quantize import get_rht_matrix\n\n    x = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n    had = get_rht_matrix()\n    x_quantized = quantize_to_fp4(x, had=had)\n    ```\n\n    ## Backends\n\n    We provide three different implementations of FP4 quantization:\n\n    - **CUDA**: A fast implementation written in CUDA which currently does not support\n        the operations required for training (2D block scaling, stochastic rounding,\n        random Hadamard transform). Requires a Blackwell GPU.\n    - **Triton**: A slightly slower implementation written in Triton which supports all\n        operations needed for training. Requires a Blackwell GPU.\n    - **PyTorch**: A slow implementation written in PyTorch which supports all\n        operations and can be run on any GPU.\n\n    If `quantize_to_fp4` is called with `backend=None`, a backend will be selected\n    automatically based on the following rules:\n\n    - If there is no GPU available, or if the available GPU is not a Blackwell GPU,\n        select PyTorch.\n    - If any quantization options are set other than `scale_rule`, select Triton.\n        - However, if the available GPU is SM120 (i.e. RTX 5090, RTX 6000) and\n            `round_style` is set to `RoundStyle.stochastic`, select PyTorch as\n            stochastic rounding does not have hardware support on SM120 GPUs.\n    - Otherwise, select CUDA.\n\n    ## Parameters\n\n    Args:\n        x (torch.Tensor): The input tensor to quantize.\n        backend (QuantizeBackend): The backend to use for quantization, either\n            `QuantizeBackend.cuda`, `QuantizeBackend.triton`, or\n            `QuantizeBackend.pytorch`. If no backend is provided, one will be selected\n            automatically based on the available GPU and the options provided. See above\n            for more details.\n        scale_rule (AdaptiveBlockScalingRule): The scaling rule to use during\n            quantization. See (Adaptive Block Scaling)[/adaptive_block_scaling] for more\n            details.\n        block_scale_2d (bool): If True, scale factors will be computed across 16x16\n            chunks of the input rather than 1x16 chunks. This is useful to apply to the\n            weight matrix during training, so that W and W.T will be equivalent after\n            quantization.\n        had (torch.Tensor): A high-precision Hadamard matrix to apply to the input prior\n            to quantization.\n        fp4_format (FP4Format): The FP4 format to quantize to, either `FP4Format.mxfp4`\n            or `FP4Format.nvfp4`.\n        round_style (RoundStyle): The rounding style to apply during quantization,\n            either `RoundStyle.nearest` for round-to-nearest quantization, or\n            `RoundStyle.stochastic` for stochastic rounding.\n        transpose (bool): If True, the output will be a quantized version of the\n            transposed input. This may be helpful for certain operations during training\n            as `fp4_matmul` requires that both tensors are provided in row-major format.\n\n    Returns:\n        A quantized FP4Tensor, which contains the packed E2M1 values, the FP8 scale\n        factors, and the tensor-wide FP32 scale factor.\n\n    \"\"\"\n\n    kwargs = {\n        \"scale_rule\": scale_rule,\n        \"block_scale_2d\": block_scale_2d,\n        \"had\": had,\n        \"fp4_format\": fp4_format,\n        \"round_style\": round_style,\n        \"transpose\": transpose,\n    }\n\n    if backend is None:\n        backend = QuantizeBackend.auto_select(x, **kwargs)\n    elif not backend.is_supported(x, **kwargs):\n        msg = f\"Backend {backend} does not support the given parameters\"\n        raise ValueError(msg)\n\n    return backend.quantize_to_fp4(x, **kwargs)\n</code></pre>"}]}