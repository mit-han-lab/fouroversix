{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Four Over Six","text":"<p>See Quantization for details on quantizing tensors to FP4, and Matrix Multiplication for performing matrix multiplication with FP4 tensors.</p>"},{"location":"matmul/","title":"Matrix Multiplication","text":"<p>Perform a matrix multiplication (<code>a @ b.T</code>) with two FP4-quantized tensors provided in row-major layout.</p>"},{"location":"matmul/#fouroversix.fp4_matmul--sample-code","title":"Sample Code","text":"<p>Each tensor may be provided in either high or low precision. If provided in high precision, tensors will be quantized to FP4 prior to the matrix multiplication, and quantization may be configured with the <code>a_quantization_kwargs</code> and <code>b_quantization_kwargs</code> parameters. For example, the following two code samples are equivalent:</p>"},{"location":"matmul/#fouroversix.fp4_matmul--with-high-precision-inputs","title":"With High-Precision Inputs","text":"<pre><code>a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\nb = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\nout = fp4_matmul(a, b)\n</code></pre>"},{"location":"matmul/#fouroversix.fp4_matmul--with-low-precision-inputs","title":"With Low-Precision Inputs","text":"<pre><code>a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\nb = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n\na_e2m1, a_sf, a_normconst = quantize_to_fp4(a)\nb_e2m1, b_sf, b_normconst = quantize_to_fp4(b)\n\nout = fp4_matmul(\n    a_e2m1=a_e2m1,\n    a_sf=a_sf,\n    a_normconst=a_normconst,\n    b_e2m1=b_e2m1,\n    b_sf=b_sf,\n    b_normconst=b_normconst\n)\n</code></pre>"},{"location":"matmul/#fouroversix.fp4_matmul--backends","title":"Backends","text":"<p>We provide two different implementations of FP4 matrix multiplication:</p> <ul> <li>CUTLASS: Uses CUTLASS kernels to perform fast FP4 matrix multiplication.     Requires a Blackwell GPU.</li> <li>PyTorch: A slow implementation which dequantizes FP4 tensors, and then     performs a high-precision matrix multiplication.</li> </ul> <p>Note that our CUTLASS kernels accumulate in FP32, so it should be roughly equivalent to simulations done with the PyTorch backend.</p>"},{"location":"matmul/#fouroversix.fp4_matmul--parameters","title":"Parameters","text":"<p>Parameters:</p> Name Type Description Default <code>a</code> <code>Tensor</code> <p>The high-precision input tensor A.</p> <code>None</code> <code>b</code> <code>Tensor</code> <p>The high-precision input tensor B.</p> <code>None</code> <code>backend</code> <code>MatmulBackend</code> <p>The backend to use for the matrix multiplication, either <code>MatmulBackend.cutlass</code> or <code>MatmulBackend.pytorch</code>. If no backend is provided, CUTLASS will be used if the machine has a Blackwell GPU, and PyTorch will be used otherwise.</p> <code>None</code> <code>a_e2m1</code> <code>Tensor</code> <p>The values of the first input tensor in packed E2M1 format (2 values per byte).</p> <code>None</code> <code>a_sf</code> <code>Tensor</code> <p>The scale factors of the first input tensor.</p> <code>None</code> <code>a_normconst</code> <code>Tensor</code> <p>The per-tensor normalization constant of the first input tensor.</p> <code>None</code> <code>b_e2m1</code> <code>Tensor</code> <p>The values of the second input tensor in packed E2M1 format (2 values per byte).</p> <code>None</code> <code>b_sf</code> <code>Tensor</code> <p>The scale factors of the second input tensor.</p> <code>None</code> <code>b_normconst</code> <code>Tensor</code> <p>The per-tensor normalization constant of the second input tensor.</p> <code>None</code> <code>a_quantize_kwargs</code> <code>dict</code> <p>If <code>a</code> is provided in high precision, these parameters will be passed to the <code>quantize_to_fp4</code> call done prior to the matrix multiplication.</p> <code>None</code> <code>b_quantize_kwargs</code> <code>dict</code> <p>If <code>b</code> is provided in high precision, these parameters will be passed to the <code>quantize_to_fp4</code> call done prior to the matrix multiplication.</p> <code>None</code> <code>fp4_format</code> <code>FP4Format</code> <p>The FP4 format of the input tensors, either <code>FP4Format.nvfp4</code> or <code>FP4Format.mxfp4</code>.</p> <code>nvfp4</code> <code>out_dtype</code> <code>DataType</code> <p>The data type of the output tensor, either <code>DataType.bfloat16</code> or <code>DataType.float16</code>.</p> <code>bfloat16</code> <code>out_shape</code> <code>tuple[int, int] | None</code> <p>The shape of the output tensor. This is helpful when the input tensors have shapes that are not multiples of 64, but which were padded to multiples of 64 during quantization.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor.</p> Source code in <code>src/fouroversix/frontend.py</code> <pre><code>def fp4_matmul(\n    a: torch.Tensor | None = None,\n    b: torch.Tensor | None = None,\n    *,\n    backend: MatmulBackend | None = None,\n    a_e2m1: torch.Tensor | None = None,\n    a_sf: torch.Tensor | None = None,\n    a_normconst: torch.Tensor | None = None,\n    b_e2m1: torch.Tensor | None = None,\n    b_sf: torch.Tensor | None = None,\n    b_normconst: torch.Tensor | None = None,\n    a_quantize_kwargs: dict[str, Any] | None = None,\n    b_quantize_kwargs: dict[str, Any] | None = None,\n    fp4_format: FP4Format = FP4Format.nvfp4,\n    out_dtype: DataType = DataType.bfloat16,\n    out_shape: tuple[int, int] | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Perform a matrix multiplication (`a @ b.T`) with two FP4-quantized tensors provided\n    in row-major layout.\n\n    ## Sample Code\n\n    Each tensor may be provided in either high or low precision. If provided in high\n    precision, tensors will be quantized to FP4 prior to the matrix multiplication, and\n    quantization may be configured with the `a_quantization_kwargs` and\n    `b_quantization_kwargs` parameters. For example, the following two code samples are\n    equivalent:\n\n    ### With High-Precision Inputs\n\n    ```\n    a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n    b = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n    out = fp4_matmul(a, b)\n    ```\n\n    ### With Low-Precision Inputs\n\n    ```\n    a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n    b = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n\n    a_e2m1, a_sf, a_normconst = quantize_to_fp4(a)\n    b_e2m1, b_sf, b_normconst = quantize_to_fp4(b)\n\n    out = fp4_matmul(\n        a_e2m1=a_e2m1,\n        a_sf=a_sf,\n        a_normconst=a_normconst,\n        b_e2m1=b_e2m1,\n        b_sf=b_sf,\n        b_normconst=b_normconst\n    )\n    ```\n\n    ## Backends\n\n    We provide two different implementations of FP4 matrix multiplication:\n\n    - **CUTLASS**: Uses CUTLASS kernels to perform fast FP4 matrix multiplication.\n        Requires a Blackwell GPU.\n    - **PyTorch**: A slow implementation which dequantizes FP4 tensors, and then\n        performs a high-precision matrix multiplication.\n\n    Note that our CUTLASS kernels accumulate in FP32, so it should be roughly\n    equivalent to simulations done with the PyTorch backend.\n\n    ## Parameters\n\n    Args:\n        a (torch.Tensor): The high-precision input tensor A.\n        b (torch.Tensor): The high-precision input tensor B.\n        backend (MatmulBackend): The backend to use for the matrix multiplication,\n            either `MatmulBackend.cutlass` or `MatmulBackend.pytorch`. If no backend is\n            provided, CUTLASS will be used if the machine has a Blackwell GPU, and\n            PyTorch will be used otherwise.\n        a_e2m1 (torch.Tensor): The values of the first input tensor in packed E2M1\n            format (2 values per byte).\n        a_sf (torch.Tensor): The scale factors of the first input tensor.\n        a_normconst (torch.Tensor): The per-tensor normalization constant of the\n            first input tensor.\n        b_e2m1 (torch.Tensor): The values of the second input tensor in packed E2M1\n            format (2 values per byte).\n        b_sf (torch.Tensor): The scale factors of the second input tensor.\n        b_normconst (torch.Tensor): The per-tensor normalization constant of the\n            second input tensor.\n        a_quantize_kwargs (dict): If `a` is provided in high precision, these parameters\n            will be passed to the `quantize_to_fp4` call done prior to the matrix\n            multiplication.\n        b_quantize_kwargs (dict): If `b` is provided in high precision, these parameters\n            will be passed to the `quantize_to_fp4` call done prior to the matrix\n            multiplication.\n        fp4_format (FP4Format): The FP4 format of the input tensors, either\n            `FP4Format.nvfp4` or `FP4Format.mxfp4`.\n        out_dtype (DataType): The data type of the output tensor, either\n            `DataType.bfloat16` or `DataType.float16`.\n        out_shape (tuple[int, int] | None): The shape of the output tensor. This is\n            helpful when the input tensors have shapes that are not multiples of 64,\n            but which were padded to multiples of 64 during quantization.\n\n    Returns:\n        The output tensor.\n\n    \"\"\"\n\n    if a is None and (a_e2m1 is None or a_sf is None):\n        msg = \"If a is None, a_e2m1 and a_sf must be provided\"\n        raise ValueError(msg)\n\n    if b is None and (b_e2m1 is None or b_sf is None):\n        msg = \"If b is None, b_e2m1 and b_sf must be provided\"\n        raise ValueError(msg)\n\n    if a_quantize_kwargs is None:\n        a_quantize_kwargs = {}\n\n    if b_quantize_kwargs is None:\n        b_quantize_kwargs = {}\n\n    if a_e2m1 is None or a_sf is None:\n        a_e2m1, a_sf, a_normconst = quantize_to_fp4(a, **a_quantize_kwargs)\n\n    if b_e2m1 is None or b_sf is None:\n        b_e2m1, b_sf, b_normconst = quantize_to_fp4(b, **b_quantize_kwargs)\n\n    kwargs = {\n        \"fp4_format\": fp4_format,\n        \"out_dtype\": out_dtype,\n        \"out_shape\": out_shape,\n    }\n\n    if backend is None:\n        backend = MatmulBackend.auto_select()\n    elif not backend.is_available():\n        msg = f\"Backend {backend} is not available\"\n        raise ValueError(msg)\n\n    return backend.fp4_matmul(\n        a_e2m1,\n        a_sf,\n        a_normconst,\n        b_e2m1,\n        b_sf,\n        b_normconst,\n        **kwargs,\n    )\n</code></pre>"},{"location":"quantization/","title":"Quantization","text":"<p>Quantize a tensor to FP4.</p>"},{"location":"quantization/#fouroversix.quantize_to_fp4--sample-code","title":"Sample Code","text":""},{"location":"quantization/#fouroversix.quantize_to_fp4--with-four-over-six","title":"With Four Over Six","text":"<pre><code>a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\na_e2m1, a_sf, a_normconst = quantize_to_fp4(a)\n</code></pre>"},{"location":"quantization/#fouroversix.quantize_to_fp4--without-four-over-six","title":"Without Four Over Six","text":"<pre><code>a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\na_e2m1, a_sf, a_normconst = quantize_to_fp4(\n    a,\n    scale_rule=AdaptiveBlockScalingRule.always_6,\n)\n</code></pre>"},{"location":"quantization/#fouroversix.quantize_to_fp4--with-stochastic-rounding","title":"With Stochastic Rounding","text":"<pre><code>a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\na_e2m1, a_sf, a_normconst = quantize_to_fp4(a, round_style=RoundStyle.stochastic)\n</code></pre>"},{"location":"quantization/#fouroversix.quantize_to_fp4--with-the-random-hadamard-transform","title":"With the Random Hadamard Transform","text":"<pre><code>from scipy.linalg import hadamard\n\na = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\nhad = torch.tensor(hadamard(16), dtype=torch.bfloat16, device=\"cuda\")\na_e2m1, a_sf, a_normconst = quantize_to_fp4(a, had=had)\n</code></pre>"},{"location":"quantization/#fouroversix.quantize_to_fp4--backends","title":"Backends","text":"<p>We provide three different implementations of FP4 quantization:</p> <ul> <li>CUDA: A fast implementation written in CUDA which currently does not support     the operations required for training (2D block scaling, stochastic rounding,     random Hadamard transform). Requires a Blackwell GPU.</li> <li>Triton: A slightly slower implementation written in Triton which supports all     operations needed for training. Requires a Blackwell GPU.</li> <li>PyTorch: A slow implementation written in PyTorch which supports all     operations and can be run on any GPU.</li> </ul> <p>If <code>quantize_to_fp4</code> is called with <code>backend=None</code>, a backend will be selected automatically based on the following rules:</p> <ul> <li>If there is no GPU available, or if the available GPU is not a Blackwell GPU,     select PyTorch.</li> <li>If any quantization options are set other than <code>scale_rule</code>, select Triton.<ul> <li>However, if the available GPU is SM120 (i.e. RTX 5090, RTX 6000) and     <code>round_style</code> is set to <code>RoundStyle.stochastic</code>, select PyTorch as     stochastic rounding does not have hardware support on SM120 GPUs.</li> </ul> </li> <li>Otherwise, select CUDA.</li> </ul>"},{"location":"quantization/#fouroversix.quantize_to_fp4--parameters","title":"Parameters","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to quantize.</p> required <code>backend</code> <code>QuantizeBackend</code> <p>The backend to use for quantization, either <code>QuantizeBackend.cuda</code>, <code>QuantizeBackend.triton</code>, or <code>QuantizeBackend.pytorch</code>. If no backend is provided, one will be selected automatically based on the available GPU and the options provided. See above for more details.</p> <code>None</code> <code>scale_rule</code> <code>AdaptiveBlockScalingRule</code> <p>The scaling rule to use during quantization. See (Adaptive Block Scaling)[/adaptive_block_scaling] for more details.</p> <code>mse</code> <code>block_scale_2d</code> <code>bool</code> <p>If True, scale factors will be computed across 16x16 chunks of the input rather than 1x16 chunks. This is useful to apply to the weight matrix during training, so that W and W.T will be equivalent after quantization.</p> <code>False</code> <code>had</code> <code>Tensor</code> <p>A high-precision Hadamard matrix to apply to the input prior to quantization.</p> <code>None</code> <code>fp4_format</code> <code>FP4Format</code> <p>The FP4 format to quantize to, either <code>FP4Format.mxfp4</code> or <code>FP4Format.nvfp4</code>.</p> <code>nvfp4</code> <code>round_style</code> <code>RoundStyle</code> <p>The rounding style to apply during quantization, either <code>RoundStyle.nearest</code> for round-to-nearest quantization, or <code>RoundStyle.stochastic</code> for stochastic rounding.</p> <code>nearest</code> <code>transpose</code> <code>bool</code> <p>If True, the output will be a quantized version of the transposed input. This may be helpful for certain operations during training as <code>fp4_matmul</code> requires that both tensors are provided in row-major format.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The packed E2M1 values.</p> <code>Tensor</code> <p>The FP8 scale factors.</p> <code>Tensor | None</code> <p>The tensor-wide FP32 scale factor.</p> Source code in <code>src/fouroversix/frontend.py</code> <pre><code>def quantize_to_fp4(\n    x: torch.Tensor,\n    *,\n    backend: QuantizeBackend | None = None,\n    scale_rule: AdaptiveBlockScalingRule = AdaptiveBlockScalingRule.mse,\n    block_scale_2d: bool = False,\n    had: torch.Tensor | None = None,\n    fp4_format: FP4Format = FP4Format.nvfp4,\n    round_style: RoundStyle = RoundStyle.nearest,\n    transpose: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor | None]:\n    \"\"\"\n    Quantize a tensor to FP4.\n\n    ## Sample Code\n\n    ### With Four Over Six\n\n    ```\n    a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n    a_e2m1, a_sf, a_normconst = quantize_to_fp4(a)\n    ```\n\n    ### Without Four Over Six\n\n    ```\n    a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n    a_e2m1, a_sf, a_normconst = quantize_to_fp4(\n        a,\n        scale_rule=AdaptiveBlockScalingRule.always_6,\n    )\n    ```\n\n    ### With Stochastic Rounding\n\n    ```\n    a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n    a_e2m1, a_sf, a_normconst = quantize_to_fp4(a, round_style=RoundStyle.stochastic)\n    ```\n\n    ### With the Random Hadamard Transform\n\n    ```\n    from scipy.linalg import hadamard\n\n    a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=\"cuda\")\n    had = torch.tensor(hadamard(16), dtype=torch.bfloat16, device=\"cuda\")\n    a_e2m1, a_sf, a_normconst = quantize_to_fp4(a, had=had)\n    ```\n\n    ## Backends\n\n    We provide three different implementations of FP4 quantization:\n\n    - **CUDA**: A fast implementation written in CUDA which currently does not support\n        the operations required for training (2D block scaling, stochastic rounding,\n        random Hadamard transform). Requires a Blackwell GPU.\n    - **Triton**: A slightly slower implementation written in Triton which supports all\n        operations needed for training. Requires a Blackwell GPU.\n    - **PyTorch**: A slow implementation written in PyTorch which supports all\n        operations and can be run on any GPU.\n\n    If `quantize_to_fp4` is called with `backend=None`, a backend will be selected\n    automatically based on the following rules:\n\n    - If there is no GPU available, or if the available GPU is not a Blackwell GPU,\n        select PyTorch.\n    - If any quantization options are set other than `scale_rule`, select Triton.\n        - However, if the available GPU is SM120 (i.e. RTX 5090, RTX 6000) and\n            `round_style` is set to `RoundStyle.stochastic`, select PyTorch as\n            stochastic rounding does not have hardware support on SM120 GPUs.\n    - Otherwise, select CUDA.\n\n    ## Parameters\n\n    Args:\n        x (torch.Tensor): The input tensor to quantize.\n        backend (QuantizeBackend): The backend to use for quantization, either\n            `QuantizeBackend.cuda`, `QuantizeBackend.triton`, or\n            `QuantizeBackend.pytorch`. If no backend is provided, one will be selected\n            automatically based on the available GPU and the options provided. See above\n            for more details.\n        scale_rule (AdaptiveBlockScalingRule): The scaling rule to use during\n            quantization. See (Adaptive Block Scaling)[/adaptive_block_scaling] for more\n            details.\n        block_scale_2d (bool): If True, scale factors will be computed across 16x16\n            chunks of the input rather than 1x16 chunks. This is useful to apply to the\n            weight matrix during training, so that W and W.T will be equivalent after\n            quantization.\n        had (torch.Tensor): A high-precision Hadamard matrix to apply to the input prior\n            to quantization.\n        fp4_format (FP4Format): The FP4 format to quantize to, either `FP4Format.mxfp4`\n            or `FP4Format.nvfp4`.\n        round_style (RoundStyle): The rounding style to apply during quantization,\n            either `RoundStyle.nearest` for round-to-nearest quantization, or\n            `RoundStyle.stochastic` for stochastic rounding.\n        transpose (bool): If True, the output will be a quantized version of the\n            transposed input. This may be helpful for certain operations during training\n            as `fp4_matmul` requires that both tensors are provided in row-major format.\n\n    Returns:\n        The packed E2M1 values.\n        The FP8 scale factors.\n        The tensor-wide FP32 scale factor.\n\n    \"\"\"\n\n    kwargs = {\n        \"scale_rule\": scale_rule,\n        \"block_scale_2d\": block_scale_2d,\n        \"had\": had,\n        \"fp4_format\": fp4_format,\n        \"round_style\": round_style,\n        \"transpose\": transpose,\n    }\n\n    if backend is None:\n        backend = QuantizeBackend.auto_select(x, **kwargs)\n    elif not backend.is_supported(x, **kwargs):\n        msg = f\"Backend {backend} does not support the given parameters\"\n        raise ValueError(msg)\n\n    return backend.quantize_to_fp4(x, **kwargs)\n</code></pre>"}]}