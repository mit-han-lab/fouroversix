
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../quantization/">
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Matrix Multiplication - Four Over Six Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#matrix-multiplication" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Four Over Six Documentation" class="md-header__button md-logo" aria-label="Four Over Six Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Four Over Six Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Matrix Multiplication
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Four Over Six Documentation" class="md-nav__button md-logo" aria-label="Four Over Six Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Four Over Six Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Four Over Six
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Matrix Multiplication
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Matrix Multiplication
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#fouroversix.fp4_matmul" class="md-nav__link">
    <span class="md-ellipsis">
      
        fp4_matmul
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="fp4_matmul">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fouroversix.fp4_matmul--sample-code" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sample Code
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Sample Code">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fouroversix.fp4_matmul--with-high-precision-inputs" class="md-nav__link">
    <span class="md-ellipsis">
      
        With High-Precision Inputs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fouroversix.fp4_matmul--with-low-precision-inputs" class="md-nav__link">
    <span class="md-ellipsis">
      
        With Low-Precision Inputs
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fouroversix.fp4_matmul--backends" class="md-nav__link">
    <span class="md-ellipsis">
      
        Backends
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fouroversix.fp4_matmul--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Quantization
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#fouroversix.fp4_matmul" class="md-nav__link">
    <span class="md-ellipsis">
      
        fp4_matmul
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="fp4_matmul">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fouroversix.fp4_matmul--sample-code" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sample Code
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Sample Code">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fouroversix.fp4_matmul--with-high-precision-inputs" class="md-nav__link">
    <span class="md-ellipsis">
      
        With High-Precision Inputs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fouroversix.fp4_matmul--with-low-precision-inputs" class="md-nav__link">
    <span class="md-ellipsis">
      
        With Low-Precision Inputs
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fouroversix.fp4_matmul--backends" class="md-nav__link">
    <span class="md-ellipsis">
      
        Backends
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fouroversix.fp4_matmul--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="matrix-multiplication">Matrix Multiplication</h1>


<div class="doc doc-object doc-function">



<a id="fouroversix.fp4_matmul"></a>
    <div class="doc doc-contents first">

        <p>Perform a matrix multiplication (<code>a @ b.T</code>) with two FP4-quantized tensors provided
in row-major layout.</p>
<h3 id="fouroversix.fp4_matmul--sample-code">Sample Code</h3>
<p>Each tensor may be provided in either high or low precision. If provided in high
precision, tensors will be quantized to FP4 prior to the matrix multiplication, and
quantization may be configured with the <code>a_quantization_kwargs</code> and
<code>b_quantization_kwargs</code> parameters. For example, the following two code samples are
equivalent:</p>
<h4 id="fouroversix.fp4_matmul--with-high-precision-inputs">With High-Precision Inputs</h4>
<pre><code>a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=&quot;cuda&quot;)
b = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=&quot;cuda&quot;)
out = fp4_matmul(a, b)
</code></pre>
<h4 id="fouroversix.fp4_matmul--with-low-precision-inputs">With Low-Precision Inputs</h4>
<pre><code>a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=&quot;cuda&quot;)
b = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=&quot;cuda&quot;)

a_e2m1, a_sf, a_normconst = quantize_to_fp4(a)
b_e2m1, b_sf, b_normconst = quantize_to_fp4(b)

out = fp4_matmul(
    a_e2m1=a_e2m1,
    a_sf=a_sf,
    a_normconst=a_normconst,
    b_e2m1=b_e2m1,
    b_sf=b_sf,
    b_normconst=b_normconst
)
</code></pre>
<h3 id="fouroversix.fp4_matmul--backends">Backends</h3>
<p>We provide two different implementations of FP4 matrix multiplication:</p>
<ul>
<li><strong>CUTLASS</strong>: Uses CUTLASS kernels to perform fast FP4 matrix multiplication.
    Requires a Blackwell GPU.</li>
<li><strong>PyTorch</strong>: A slow implementation which dequantizes FP4 tensors, and then
    performs a high-precision matrix multiplication.</li>
</ul>
<p>Note that our CUTLASS kernels accumulate in FP32, so it should be roughly
equivalent to simulations done with the PyTorch backend.</p>
<h3 id="fouroversix.fp4_matmul--parameters">Parameters</h3>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>a</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The high-precision input tensor A.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>b</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The high-precision input tensor B.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>backend</code>
            </td>
            <td>
                  <code><span title="fouroversix.backend.MatmulBackend">MatmulBackend</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The backend to use for the matrix multiplication,
either <code>MatmulBackend.cutlass</code> or <code>MatmulBackend.pytorch</code>. If no backend is
provided, CUTLASS will be used if the machine has a Blackwell GPU, and
PyTorch will be used otherwise.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>a_e2m1</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The values of the first input tensor in packed E2M1
format (2 values per byte).</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>a_sf</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The scale factors of the first input tensor.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>a_normconst</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The per-tensor normalization constant of the
first input tensor.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>b_e2m1</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The values of the second input tensor in packed E2M1
format (2 values per byte).</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>b_sf</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The scale factors of the second input tensor.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>b_normconst</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The per-tensor normalization constant of the
second input tensor.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>a_quantize_kwargs</code>
            </td>
            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>a</code> is provided in high precision, these parameters
will be passed to the <code>quantize_to_fp4</code> call done prior to the matrix
multiplication.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>b_quantize_kwargs</code>
            </td>
            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>b</code> is provided in high precision, these parameters
will be passed to the <code>quantize_to_fp4</code> call done prior to the matrix
multiplication.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>fp4_format</code>
            </td>
            <td>
                  <code><span title="fouroversix.utils.FP4Format">FP4Format</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The FP4 format of the input tensors, either
<code>FP4Format.nvfp4</code> or <code>FP4Format.mxfp4</code>.</p>
              </div>
            </td>
            <td>
                  <code><span title="fouroversix.utils.FP4Format.nvfp4">nvfp4</span></code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_dtype</code>
            </td>
            <td>
                  <code><span title="fouroversix.utils.DataType">DataType</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The data type of the output tensor, either
<code>DataType.bfloat16</code> or <code>DataType.float16</code>.</p>
              </div>
            </td>
            <td>
                  <code><span title="fouroversix.utils.DataType.bfloat16">bfloat16</span></code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_shape</code>
            </td>
            <td>
                  <code><span title="tuple">tuple</span>[<span title="int">int</span>, <span title="int">int</span>] | None</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The shape of the output tensor. This is
helpful when the input tensors have shapes that are not multiples of 64,
but which were padded to multiples of 64 during quantization.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The output tensor.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/fouroversix/frontend.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fp4_matmul</span><span class="p">(</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="n">MatmulBackend</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">a_e2m1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">a_sf</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">a_normconst</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">b_e2m1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">b_sf</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">b_normconst</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">a_quantize_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">b_quantize_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fp4_format</span><span class="p">:</span> <span class="n">FP4Format</span> <span class="o">=</span> <span class="n">FP4Format</span><span class="o">.</span><span class="n">nvfp4</span><span class="p">,</span>
    <span class="n">out_dtype</span><span class="p">:</span> <span class="n">DataType</span> <span class="o">=</span> <span class="n">DataType</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="n">out_shape</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform a matrix multiplication (`a @ b.T`) with two FP4-quantized tensors provided</span>
<span class="sd">    in row-major layout.</span>

<span class="sd">    ## Sample Code</span>

<span class="sd">    Each tensor may be provided in either high or low precision. If provided in high</span>
<span class="sd">    precision, tensors will be quantized to FP4 prior to the matrix multiplication, and</span>
<span class="sd">    quantization may be configured with the `a_quantization_kwargs` and</span>
<span class="sd">    `b_quantization_kwargs` parameters. For example, the following two code samples are</span>
<span class="sd">    equivalent:</span>

<span class="sd">    ### With High-Precision Inputs</span>

<span class="sd">    ```</span>
<span class="sd">    a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=&quot;cuda&quot;)</span>
<span class="sd">    b = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=&quot;cuda&quot;)</span>
<span class="sd">    out = fp4_matmul(a, b)</span>
<span class="sd">    ```</span>

<span class="sd">    ### With Low-Precision Inputs</span>

<span class="sd">    ```</span>
<span class="sd">    a = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=&quot;cuda&quot;)</span>
<span class="sd">    b = torch.tensor(1024, 1024, dtype=torch.bfloat16, device=&quot;cuda&quot;)</span>

<span class="sd">    a_e2m1, a_sf, a_normconst = quantize_to_fp4(a)</span>
<span class="sd">    b_e2m1, b_sf, b_normconst = quantize_to_fp4(b)</span>

<span class="sd">    out = fp4_matmul(</span>
<span class="sd">        a_e2m1=a_e2m1,</span>
<span class="sd">        a_sf=a_sf,</span>
<span class="sd">        a_normconst=a_normconst,</span>
<span class="sd">        b_e2m1=b_e2m1,</span>
<span class="sd">        b_sf=b_sf,</span>
<span class="sd">        b_normconst=b_normconst</span>
<span class="sd">    )</span>
<span class="sd">    ```</span>

<span class="sd">    ## Backends</span>

<span class="sd">    We provide two different implementations of FP4 matrix multiplication:</span>

<span class="sd">    - **CUTLASS**: Uses CUTLASS kernels to perform fast FP4 matrix multiplication.</span>
<span class="sd">        Requires a Blackwell GPU.</span>
<span class="sd">    - **PyTorch**: A slow implementation which dequantizes FP4 tensors, and then</span>
<span class="sd">        performs a high-precision matrix multiplication.</span>

<span class="sd">    Note that our CUTLASS kernels accumulate in FP32, so it should be roughly</span>
<span class="sd">    equivalent to simulations done with the PyTorch backend.</span>

<span class="sd">    ## Parameters</span>

<span class="sd">    Args:</span>
<span class="sd">        a (torch.Tensor): The high-precision input tensor A.</span>
<span class="sd">        b (torch.Tensor): The high-precision input tensor B.</span>
<span class="sd">        backend (MatmulBackend): The backend to use for the matrix multiplication,</span>
<span class="sd">            either `MatmulBackend.cutlass` or `MatmulBackend.pytorch`. If no backend is</span>
<span class="sd">            provided, CUTLASS will be used if the machine has a Blackwell GPU, and</span>
<span class="sd">            PyTorch will be used otherwise.</span>
<span class="sd">        a_e2m1 (torch.Tensor): The values of the first input tensor in packed E2M1</span>
<span class="sd">            format (2 values per byte).</span>
<span class="sd">        a_sf (torch.Tensor): The scale factors of the first input tensor.</span>
<span class="sd">        a_normconst (torch.Tensor): The per-tensor normalization constant of the</span>
<span class="sd">            first input tensor.</span>
<span class="sd">        b_e2m1 (torch.Tensor): The values of the second input tensor in packed E2M1</span>
<span class="sd">            format (2 values per byte).</span>
<span class="sd">        b_sf (torch.Tensor): The scale factors of the second input tensor.</span>
<span class="sd">        b_normconst (torch.Tensor): The per-tensor normalization constant of the</span>
<span class="sd">            second input tensor.</span>
<span class="sd">        a_quantize_kwargs (dict): If `a` is provided in high precision, these parameters</span>
<span class="sd">            will be passed to the `quantize_to_fp4` call done prior to the matrix</span>
<span class="sd">            multiplication.</span>
<span class="sd">        b_quantize_kwargs (dict): If `b` is provided in high precision, these parameters</span>
<span class="sd">            will be passed to the `quantize_to_fp4` call done prior to the matrix</span>
<span class="sd">            multiplication.</span>
<span class="sd">        fp4_format (FP4Format): The FP4 format of the input tensors, either</span>
<span class="sd">            `FP4Format.nvfp4` or `FP4Format.mxfp4`.</span>
<span class="sd">        out_dtype (DataType): The data type of the output tensor, either</span>
<span class="sd">            `DataType.bfloat16` or `DataType.float16`.</span>
<span class="sd">        out_shape (tuple[int, int] | None): The shape of the output tensor. This is</span>
<span class="sd">            helpful when the input tensors have shapes that are not multiples of 64,</span>
<span class="sd">            but which were padded to multiples of 64 during quantization.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The output tensor.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">a</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">a_e2m1</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">a_sf</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;If a is None, a_e2m1 and a_sf must be provided&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">b_e2m1</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">b_sf</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;If b is None, b_e2m1 and b_sf must be provided&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">a_quantize_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">a_quantize_kwargs</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="n">b_quantize_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_quantize_kwargs</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="n">a_e2m1</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">a_sf</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">a_e2m1</span><span class="p">,</span> <span class="n">a_sf</span><span class="p">,</span> <span class="n">a_normconst</span> <span class="o">=</span> <span class="n">quantize_to_fp4</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="o">**</span><span class="n">a_quantize_kwargs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">b_e2m1</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">b_sf</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_e2m1</span><span class="p">,</span> <span class="n">b_sf</span><span class="p">,</span> <span class="n">b_normconst</span> <span class="o">=</span> <span class="n">quantize_to_fp4</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="o">**</span><span class="n">b_quantize_kwargs</span><span class="p">)</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;fp4_format&quot;</span><span class="p">:</span> <span class="n">fp4_format</span><span class="p">,</span>
        <span class="s2">&quot;out_dtype&quot;</span><span class="p">:</span> <span class="n">out_dtype</span><span class="p">,</span>
        <span class="s2">&quot;out_shape&quot;</span><span class="p">:</span> <span class="n">out_shape</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="n">backend</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">MatmulBackend</span><span class="o">.</span><span class="n">auto_select</span><span class="p">()</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">backend</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Backend </span><span class="si">{</span><span class="n">backend</span><span class="si">}</span><span class="s2"> is not available&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">backend</span><span class="o">.</span><span class="n">fp4_matmul</span><span class="p">(</span>
        <span class="n">a_e2m1</span><span class="p">,</span>
        <span class="n">a_sf</span><span class="p">,</span>
        <span class="n">a_normconst</span><span class="p">,</span>
        <span class="n">b_e2m1</span><span class="p">,</span>
        <span class="n">b_sf</span><span class="p">,</span>
        <span class="n">b_normconst</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": [], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
    
  </body>
</html>